{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nilearn import image\n",
    "from nilearn import input_data\n",
    "from nilearn import signal\n",
    "from nilearn import connectome\n",
    "\n",
    "from bids import BIDSLayout\n",
    "from load_confounds import Params9\n",
    "from load_confounds import AnatCompCor\n",
    "\n",
    "import process_bids as pb\n",
    "import process_connectome as pc\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs: dataset.\n",
    "dataset_name = 'msc'\n",
    "dataset_path = os.path.abspath('../msc-raw')\n",
    "dataset_derivatives_path = os.path.abspath(r'../msc-derivatives/fmriprep')\n",
    "\n",
    "# Inputs: brain parcellation.\n",
    "atlas_name = 'CAB-NP'\n",
    "atlas_path = os.path.abspath(r'../atlas/CAB-NP_volumetric/CAB-NP_volumetric_liberal.nii.gz')\n",
    "\n",
    "# Settings.\n",
    "pybids_cache_path = 'pybids_True.sql'\n",
    "reset_pybids_cache = False\n",
    "validate_bids = True\n",
    "confounds_algorithms = {\n",
    "    'None': None,\n",
    "    'params9': Params9(),\n",
    "    'aCompCor': AnatCompCor()\n",
    "}\n",
    "\n",
    "# Outputs.\n",
    "denoised_output_dir_tpl = f'{dataset_name}-derivatives/denoised-v0.01/sub-{{sub_id}}/'\n",
    "denoised_output_csv_tpl = f'{dataset_name}-derivatives/denoised-v0.01/sub-{{sub_id}}/{{atlas_name}}-denoised-timeseries.csv'\n",
    "denoised_output_info_csv = f'{dataset_name}-derivatives/denoised-v0.01/denoised-timeseries-info.csv'\n",
    "connectomes_output_dir_tpl = f'{dataset_name}-derivatives/connectomes-v0.01/sub-{{sub_id}}/'\n",
    "connectomes_output_csv_tpl = f'{dataset_name}-derivatives/connectomes-v0.01/sub-{{sub_id}}/{{atlas_name}}-{{confound-name}}-{{caract}}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load input dataset bids structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BIDS Layout: ...D:\\msc-raw | Subjects: 0 | Sessions: 0 | Runs: 0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lecture du format BIDS\n",
    "layout = BIDSLayout(dataset_path, derivatives = dataset_derivatives_path, validate=validate_bids,\n",
    "                    database_path=pybids_cache_path, index_metadata=True, reset_database=reset_pybids_cache)\n",
    "\n",
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load input dataset subject list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects ID: ['MSC01', 'MSC02', 'MSC03', 'MSC04', 'MSC05', 'MSC06', 'MSC07', 'MSC08', 'MSC09', 'MSC10']\n"
     ]
    }
   ],
   "source": [
    "sub_ids = layout.get_subjects()\n",
    "print('Subjects ID:', sub_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Parallel processing for time series denoising calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "time_series_info = pd.DataFrame(columns = sub_ids, index = ['path','nlevels','atlas', 'tr'])\n",
    "time_series_info.loc['atlas', :] = atlas_name\n",
    "\n",
    "def get_tr_by_subject(sub_id):\n",
    "    try:\n",
    "        return layout.get_tr(subject=sub_id)\n",
    "    except Exception:\n",
    "        return None\n",
    "time_series_info.loc['tr', :] = time_series_info.columns.map(get_tr_by_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for sub_id in sub_ids:\n",
    "    denoised_output_csv = denoised_output_csv_tpl.format(sub_id=sub_id, atlas_name=atlas_name)\n",
    "    \n",
    "    ses_ids = pb.get_sessions(layout, sub_id)\n",
    "    task_ids_by_ses_ids = pb.get_tasks(layout, sub_id, ses_ids)\n",
    "    run_ids_by_tasks_ids_by_ses_ids = pb.get_run(layout, sub_id, ses_ids, task_ids_by_ses_ids)\n",
    "    sub_timeseries_df = pc.create_empty_df_timesseries(ses_ids, task_ids_by_ses_ids, run_ids_by_tasks_ids_by_ses_ids, confounds_algorithms.keys())\n",
    "\n",
    "    # Calcul sur l'ensemble des nifti\n",
    "    sub_bold_nifti_paths = layout.get(subject=sub_id, extension='nii.gz', suffix='bold', scope='derivatives',\n",
    "                                  return_type='filename')\n",
    "\n",
    "    for confounds_algorithm_name, confounds_algorithm in confounds_algorithms.items():\n",
    "        loaded_confounds = confounds_algorithm.load(sub_bold_nifti_paths)\n",
    "\n",
    "        time_series_info.loc['path', sub_id] = denoised_output_csv\n",
    "        time_series_info.loc['nlevels', sub_id] = sub_timeseries_df.columns.nlevels\n",
    "        t_r = time_series_info.loc['tr', sub_id]\n",
    "        if t_r == None:\n",
    "            print (f'error: subject {sub_id} cancelled')\n",
    "            print (f'error: multiple tr within subject')\n",
    "            continue\n",
    "        atlas_masker = input_data.NiftiLabelsMasker(atlas_path, standardize=False, smoothing_fwhm=6, low_pass=0.01, t_r=t_r)\n",
    "        sub_timeseries_df = pc.calculate_timeseries(atlas_masker, run_ids_by_tasks_ids_by_ses_ids, sub_bold_nifti_paths, denoised_output_csv,\n",
    "                                                    sub_timeseries_df, t_r, \n",
    "                                                    confound_name=confounds_algorithm_name, loaded_confounds=loaded_confounds)\n",
    "                                                    \n",
    "    time_series_info.to_csv(denoised_output_info_csv, header=True)\n",
    "    time_series_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Subjects caracteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "time_series_info = pd.read_csv(info_csv_path, header = 0, index_col=0)\n",
    "time_series_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Connectomes calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "confounds_algorithms = {\n",
    "    'params9': Params9(),\n",
    "    'aCompCor': AnatCompCor()\n",
    "}\n",
    "\n",
    "# Set pairs of atlas / confound algorithms we want to use.\n",
    "output_pairs = ((atlas_name, confounds_algorithm_name) for confounds_algorithm_name in confounds_algorithm.keys())\n",
    "for atlas_name, confounds_algorithm_name in output_pairs:\n",
    "    # Create output directory for every pair.\n",
    "    connectomes_output_dir = connectomes_output_dir_tpl.format(atlas_name=atlas_name)\n",
    "    if not os.path.exists(connectomes_output_dir):\n",
    "        os.makedirs(connectomes_output_dir)\n",
    "    \n",
    "    # Extract connectomes in the output directory using the given settings.\n",
    "    pc.extract_connectomes(\n",
    "        time_series_info,\n",
    "        connectomes_output_dir,\n",
    "        confounds_algorithm_name,\n",
    "        kind='correlation',\n",
    "        vectorize=True,\n",
    "        discard_diagonal=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
